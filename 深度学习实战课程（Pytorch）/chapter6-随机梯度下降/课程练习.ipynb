{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 以一个简单的函数为例子: y = x*w + b\n",
    "x = torch.ones(1)\n",
    "w = torch.full(size=[1], fill_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 导入函数库\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用autograd方法进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "# 均方差损失函数\n",
    "# 第一个参数是预测值，第二个参数是真实值\n",
    "mse = F.mse_loss(x*w, torch.ones(1))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 使用autograd函数对loss函数求导\n",
    "# 这里会报错,是因为我们之前没有指定w需要求导\n",
    "# torch.autograd.grad(mse, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.], requires_grad=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "w.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1., grad_fn=<MseLossBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "# 此时需要重新计算一下mse\n",
    "mse = F.mse_loss(x*w, torch.ones(1))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([2.]),)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "torch.autograd.grad(mse, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用backward方法进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1., grad_fn=<MseLossBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "mse = F.mse_loss(x*w, torch.ones(1))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 使用backward方法进行求导\n",
    "mse.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 无论使用哪种方法,都需要对参数指定需要求导信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 计算Softmax函数的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5214, 0.9971, 0.7792])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5214, 0.9971, 0.7792], requires_grad=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "# 指定需要求导信息\n",
    "a.requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.2562, 0.4122, 0.3315], grad_fn=<SoftmaxBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "p = F.softmax(a, dim=0)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 0.1906, -0.1056, -0.0849]),)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "# 使用autograd方法进行求导\n",
    "# 这里我们p[0]对a0, a1, a2的偏导\n",
    "torch.autograd.grad(p[0], a, retain_graph=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-0.1056,  0.2423, -0.1367]),)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ],
   "source": [
    "torch.autograd.grad(p[1], a, retain_graph=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-0.0849, -0.1367,  0.2216]),)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "torch.autograd.grad(p[2], a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 单层感知器的导数推导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.5173, -0.1510,  1.5163, -1.1457,  0.6900, -0.3498,  0.4459,  0.1377,\n          0.9805, -1.5648]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 20
    }
   ],
   "source": [
    "# 1行10列的数据\n",
    "x = torch.randn(1, 10)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.4973, -0.5020, -0.0952, -1.7759,  0.9548, -0.3387,  1.4977, -0.7681,\n          0.8197,  1.0211]], requires_grad=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "w = torch.randn(1, 10, requires_grad=True)\n",
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5596]], grad_fn=<SigmoidBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "# x矩阵乘以w的转置\n",
    "o = torch.sigmoid(x @ w.t())\n",
    "o\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1940, grad_fn=<MseLossBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 23
    }
   ],
   "source": [
    "# 使用均方差作为loss，假设真实值为[[1]]，计算loss，并求梯度\n",
    "# 第一个参数是预测值，第二个参数是真实值\n",
    "loss = F.mse_loss(o, torch.ones(1,1))\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 24
    }
   ],
   "source": [
    "# loss是一个标量\n",
    "loss.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 使用backward求导\n",
    "loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3294,  0.0328, -0.3291,  0.2487, -0.1498,  0.0759, -0.0968, -0.0299,\n         -0.2128,  0.3397]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 26
    }
   ],
   "source": [
    "w.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 多层感知器的导数推导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1235, -1.6236,  2.3391, -2.3477,  0.0476,  0.4647,  0.8141,  0.1049,\n          1.4921,  0.0067]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 27
    }
   ],
   "source": [
    "# 第一层有10个神经元\n",
    "x = torch.randn(1, 10)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9.2778e-01, 8.2995e-01, 9.1204e-01, 8.1783e-01, 5.5393e-01, 8.4093e-01,\n         1.8543e-01, 1.2240e-01, 5.8497e-01, 7.2318e-01],\n        [1.6900e-01, 3.0168e-01, 3.7606e-01, 4.9990e-04, 3.8866e-01, 8.9428e-01,\n         2.0580e-01, 1.9254e-01, 1.5388e-01, 8.2669e-01]], requires_grad=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 29
    }
   ],
   "source": [
    "# 第二层有2个神经元（有2*10个权重）\n",
    "# 不要忘记添加需要求导的信息\n",
    "w = torch.rand(2, 10, requires_grad=True)\n",
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6080, 0.7802]], grad_fn=<SigmoidBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 30
    }
   ],
   "source": [
    "o = torch.sigmoid(x@w.t())\n",
    "o"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1010, grad_fn=<MseLossBackward>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 31
    }
   ],
   "source": [
    "loss = F.mse_loss(o, torch.ones(1, 2))\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0115,  0.1517, -0.2185,  0.2193, -0.0044, -0.0434, -0.0760, -0.0098,\n         -0.1394, -0.0006],\n        [-0.0047,  0.0612, -0.0882,  0.0885, -0.0018, -0.0175, -0.0307, -0.0040,\n         -0.0563, -0.0003]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 33
    }
   ],
   "source": [
    "w.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 链式法则导数推导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 35
    }
   ],
   "source": [
    "x = torch.tensor(1.)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "w1 = torch.tensor(2., requires_grad=True)\n",
    "b1 = torch.tensor(1.)\n",
    "w2 = torch.tensor(2., requires_grad=True)\n",
    "b2 = torch.tensor(1.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# 定义y1\n",
    "y1 = x*w1 + b1\n",
    "# 定义y2\n",
    "y2 = y1*w2 + b2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "dy2_dy1 = torch.autograd.grad(y2, y1, retain_graph=True)[0]\n",
    "dy1_dw1 = torch.autograd.grad(y1, w1, retain_graph=True)[0]\n",
    "dy2_dw1 = torch.autograd.grad(y2, w1, retain_graph=True)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "dy2_dy1 * dy1_dw1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 48
    }
   ],
   "source": [
    "dy2_dw1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 可以看出，结果是一样的\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}