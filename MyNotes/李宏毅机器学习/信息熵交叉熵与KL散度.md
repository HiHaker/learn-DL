### 公式

$$
H(X)=-\sum_{i=1}^{n}P(x_i)logP(x_i)
$$

$P(x_i)$是随机变量$X$取$x_i$的概率。

### 通俗的解释

#### 信息量

信息量是对信息的度量，我们收到的信息量和具体发生的事件有关，**且越小概率的事件发生，我们接收到的信息量就越大。**举个例子，生活中，太阳从东边升起几乎是一个确定事件，如果有人告诉你，今天的太阳是从东边升起的，这在我们看来就是废话，我们接收到的信息量为0。而如果有人告诉你，刚刚美国和日本开战了，这是一个小概率事件，我们就能获得很大的信息量。

**所以一个具体事件的信息量随着它概率的增大而逐渐递减，且不能是负值。**

还有一个条件，假设有两个独立的事件$A$，$B$，我们观察到$AB$同时发生时获得的信息量应该等于我们分别观察到事件$A$，$B$时获得的信息量之和，也就是要满足$h(AB)=h(A)+h(B)$，那么，就可以取对数函数，事件$A$发生获得的信息量为$h(A)=log\frac{1}{P(A)}$，这里取概率的导数是为了随着概率增大，信息量减少，且因为概率的导数大于1，保证了这个值大于0。

最后，我们定义一个随机变量$X$，它包含的信息熵为随机变量$log\frac{1}{P(X)}$的数学期望，所以就等于：
$$
H(X)=\sum_{i=1}^{n}P(x_i)log\frac{1}{P(x_i)}=-\sum_{i=1}^{n}P(x_i)logP(x_i)
$$

### 解释

信息论是在探究如何把信息可靠且高效地从发送者传递到接收者，在数字时代，信息由bit编码而成，但并不是每一个bit都是有用的，如有些是多余的，有些是错误的。在我们传达信息时，我们想要尽可能地传递有用的信息。

在香农的理论中，**传递一个bit信息意味着将接收者的不确定性除以2（即不确定性降低为原来的两倍）。**

假设天气是完全随机的，明天有50%的概率是晴天，50%的概率是雨天，现在气象台告诉我们明天是雨天，就把我们的不确定性从2降为了1（之前有两个可能的结果，晴天雨天，现在只有一个），即气象台向我们传递了1个bit的有效信息。不管我们如何编码这个信息，比如用一个字符串"Rainy"，一个字符占一个字节（8bit），那么总共有40bit的信息，但真正有效的信息只有1bit。

来看看复杂一点的情况，天气还是完全随机的，但是有8种不同的天气，那么当气象台给你预报明天的天气时，就相当于将你的不确定性从8降低为了1，所以气象台向你传递了3bit有效的信息，因为$log_28=3$。

根据以上的例子我们看出，通过**计算不确定性的减少系数的对数**，如$log8$，我们就可以得到实际传递的有效信息数。

如果多个事件发生的概率不相等怎么办？比如假设明天的天气是75%晴天，25%雨天，如果气象台告诉我们明天是雨天，怎么计算传递的有效信息位数呢？**不确定性的减少系数是事件发生概率的倒数**，所以在这个例子，$\frac{1}{0.25}=4, \ \ log4=2$，所以传递的有效信息为2bit。又因为$log\frac{1}{x}=-log(x)$，这也就可以推导出：**传递信息的有效bit数等于事件发生的概率的以2为底的对数的相反数**。

那么我们实际平均能从气象站得到多少有效信息呢？$(75\% \times -log0.75) + (25\% \times -log0.25)$
$$
H(X)=-\sum_{i=1}^{n}P(x_i)logP(x_i)
$$
这样就推导出了我们一开始展示的公式，这个概念被称为**信息熵**，很好地衡量了事件的不确定性，信息熵越大，事件的不确定性也就越大。它给出了我们从一个概率分布$P$产生的样本中获得的平均信息量。

#### 交叉熵

假设一个地区的天气的真实分布如下：

![](.\imgs\l0_1.png)

下面的是对每种天气的编码，也表示我们的预测分布。举例来说，对于晴天，我们给它编码2bit，说明我们假设它出现的概率是$0.25$，所以我们得到了$-log0.25=2$，我们计算平均编码长度，也就是$35\% \times 2+35\% \times 2+10\% \times 3+10\% \times 3+4\% \times 4+4\% \times 4+1\% \times 5+1\% \times 5=2.42$，而它的信息熵可以计算出来$0.35\times-log0.35+...+0.01\times-log0.01=2.23$，两个结果很接近了，相差就等于$2.42-2.23=0.19$。

真实分布$p$与预测分布$q$之间的函数关系：
$$
H(p,q)=-\sum_{i=1}^{n}p(x_i)logq(x_i)
$$
如果预测分布与真实分布之间的差距为0，交叉熵就等于真实分布的信息熵；如果差距较大，交叉熵的值就会远远大于真实分布的信息熵的值，这个交叉熵大于真实分布信息熵的值，称为相对熵，也叫做KL Divergence。

#### KL Divergence

$$
D_{KL}(p||q)=H(p,q)-H(p)
$$

参考：

[简介机器学习中的概念：信息熵、交叉熵、KL散度](https://www.bilibili.com/video/BV1To4y1Q7jn)

[通俗理解信息熵 - 忆臻的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/26486223)

