设离散型总体$X \sim p(x;\theta)$，$\theta \in \Theta$，是未知参数，$X_1,X_2,...,X_n$是样本，它的观察值为$x_1,x_2,\dots,x_n$，则事件$\{X_1=x_1, X_2=x_2,...,X_n=x_n\}$的概率为：
$$
L(\theta)=\prod_{i=1}^{n}p(x_i;\theta)
$$
这个函数$L(\theta)$被称为**似然函数**，我们希望找到一个参数$\theta$使得我们假设的分布$p$更接近它真实的分布，因此我们希望找到一个$\theta$值使得$L(\theta)$最大（该参数对应的我们的假设的分布最有可能产生已观测的样本值）：
$$
L(\hat\theta(x_1,x_2,...,x_n))=\max_{\theta \in \Theta} L(\theta)
$$
$\hat\theta(x_1,x_2,...,x_n)$称为$\theta$的极大似然估计值，$\hat\theta(X_1,X_2,...,X_n)$称为$\theta$的极大似然估计量（MLE）。

对于连续型总体，假设其概率密度为$f(x;\theta)$，那么似然函数取：
$$
L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)
$$

- 未知参数$\theta$可能不止一个，设$\theta={\theta_1,\theta_2...,\theta_k}$

- 求$L(\theta)$的最大值，可以转换为求解$lnL(\theta)$的最大值，被称为最大似然函数。

#### 机器学习中做生成任务的极大似然估计

- 首先给定一个数据分布，$P_{data}(x)$，我们可以从其中sample出样本。

- 我们有一个假设的分布$P_G(x;\theta)$，以$\theta$为参数。

- - 我们要找到参数$\theta$以使得$P_G(x;\theta)$和$P_{data}(x)$越接近越好。
  - 举例来说，如果$P_G(x;\theta)$是高斯分布，那么$\theta$就是这个分布的均值和方差。

  我们现在从$P_{data}(x)$中sample出${x^1,x^2,...,x^m}$，我们能够计算$P_G(x^i;\theta)$，那么似然函数就是：
  $$
  L(\theta)=\prod_{i=1}^{m}P_G(x^i;\theta)
  $$
  我们想要找$\theta^*$极大化这个似然函数。

#### 极大似然估计等同于最小化KL Divergence

$$
\theta^*=arg\max_{\theta}\prod_{i=1}^{m}P_G(x^i;\theta)=arg\max_{\theta}\prod_{i=1}^{m}logP_G(x^i;\theta)
$$

因为对数函数是单增的，所以可以加上一个对数。

接下来，根据对数函数运算，$logM \cdot N=logM+logN$，（补充：$log\frac{M}{N}=logM-logN$，$logM^N=NlogM$。），式子可以进一步变为：
$$
\theta^*=arg\max_{\theta}\sum_{i=1}^{m}logP_G(x^i;\theta)
$$
因为${x^1,x^2,...,x^m}$ from $P_{data}(x)$，根据弱大数定理（辛钦大数定理）（设随机变量$X_1,X_2,...,X_n$相互独立，服从同一分布且具有数学期望$E(X_k)=\mu(k=1,2,...)$，那么序列$\bar X=\frac{1}{n}\sum_{k=1}^{n}X_k$依概率收敛于$\mu$）：
$$
\theta^*\approx arg\max_{\theta}E_{x\sim P_{data}}[logP_G(x^i;\theta)]
$$
所以：
$$
=arg\max_{\theta}\int_{x}P_{data}(x)logP_G(x;\theta)dx
$$
就等于：
$$
=arg\min_{\theta}-\int_{x}P_{data}(x)logP_G(x;\theta)dx
$$
现在我们注意到$-\int_{x}P_{data}(x)logP_G(x;\theta)dx$就是$P_{data}$和$P_G$的交叉熵，然后我们在式子后添加一项，$\int_{x}P_{data}(x)logP_{data}(x)dx$，式子就变为：
$$
=arg\min_{\theta}-\int_{x}P_{data}(x)logP_G(x;\theta)dx-[-\int_{x}P_{data}(x)logP_{data}(x)dx]
$$
就等于
$$
=arg\min_{\theta}H(P_{data},P_G)-H(P_{data})
$$
就相当于最小化$P_{data}$和$P_G$的KL Divergence：
$$
=arg\min_{\theta}KL(P_{data}||P_G)
$$
参考：

[李宏毅老师机器学习](https://www.bilibili.com/video/BV1JE411g7XF?p=75)

[极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 小纸屑的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/84764177)